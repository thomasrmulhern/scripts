{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels as sm\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer, Binarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scripts for preprocessing data for machine learning\n",
    "#Borrowed generously from https://machinelearningmastery.com/prepare-data-machine-learning-python-scikit-learn/\n",
    "\n",
    "\n",
    "def prepare_data_from_csv(file_path_as_string, y_column_name, column_names=None, header=0):\n",
    "    \n",
    "    '''Import and prepare data'''\n",
    "    \n",
    "    dataframe = pd.read_csv(file_path_as_string, names=column_names, delimiter=',',header=header)\n",
    "    if header != None:\n",
    "        dataframe.columns = [x.replace(' ', '_') for x in dataframe.columns]\n",
    "    print(dataframe.head())\n",
    "    \n",
    "    X = dataframe.drop(y_column_name, axis=1)\n",
    "    y = dataframe[y_column_name]\n",
    "    \n",
    "    prepared_df = dataframe\n",
    "    return prepared_df, X, y\n",
    "\n",
    "\n",
    "def describe_data(prepared_df):\n",
    "    \n",
    "    ''' Print shape and descriptive statistics'''\n",
    "    \n",
    "    print('Shape: ','\\n'+'--'*25 + f'\\n{prepared_df.shape}')\n",
    "    print('--'*25)\n",
    "    print('Nulls: ', '\\n'+'--'*25 + f'\\n{prepared_df.isnull().sum()}')\n",
    "    print('--'*25, '\\n'+'--'*25)\n",
    "    print('Describe: ', '\\n'+'--'*25 + f'\\n{prepared_df.describe()}')\n",
    "    print('--'*25, '\\n'+'--'*25)\n",
    "    \n",
    "def rescale_data(X): \n",
    "    \n",
    "    '''When your data is comprised of attributes with varying scales, many machine learning algorithms can benefit \n",
    "    from rescaling the attributes to all have the same scale.Often this is referred to as normalization and attributes\n",
    "    are often rescaled into the range between 0 and 1. This is useful for optimization algorithms in used in the core\n",
    "    of machine learning algorithms like gradient descent. It is also useful for algorithms that weight inputs like \n",
    "    regression and neural networks and algorithms that use distance measures like K-Nearest Neighbors.\n",
    "    \n",
    "    Input: dataframe data to be used for features\n",
    "    Return: scaled data\n",
    "'''\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    rescaledX = scaler.fit_transform(X)\n",
    "    \n",
    "    # summarize transformed data\n",
    "    np.set_printoptions(precision=3)\n",
    "    print(rescaledX[0:5,:])\n",
    "    \n",
    "    return rescaledX\n",
    "\n",
    "\n",
    "def standardize_data(X):\n",
    "    \n",
    "    '''Standardization is a useful technique to transform attributes with a Gaussian distribution and differing \n",
    "    means and standard deviations to a standard Gaussian distribution with a mean of 0 and a standard deviation of \n",
    "    1.It is most suitable for techniques that assume a Gaussian distribution in the input variables and work better \n",
    "    with rescaled data, such as linear regression, logistic regression and linear discriminate analysis.\n",
    "    \n",
    "    Input: dataframe data to be used for features\n",
    "    Return: standardized data\n",
    "    '''\n",
    "    \n",
    "    stand_scaler = StandardScaler().fit(X)\n",
    "    stand_rescaledX = stand_scaler.transform(X)\n",
    "    \n",
    "    # summarize transformed data\n",
    "    np.set_printoptions(precision=3)\n",
    "    print(stand_rescaledX[0:5,:])\n",
    "    \n",
    "    return stand_rescaledX\n",
    "\n",
    "    \n",
    "    \n",
    "def normalize_data(X):\n",
    "    \n",
    "    '''rescaling each observation (row) to have a length of 1 (called a unit norm in linear algebra). This \n",
    "    preprocessing can be useful for sparse datasets (lots of zeros) with attributes of varying scales when \n",
    "    using algorithms that weight input values such as neural networks and algorithms that use distance measures \n",
    "    such as K-Nearest Neighbors.\n",
    "    \n",
    "    Input: dataframe data to be used for features\n",
    "    Return: normalized data'''\n",
    "    \n",
    "    norm_scaler = Normalizer().fit(X)\n",
    "    normalizedX = norm_scaler.transform(X)\n",
    "    \n",
    "    # summarize transformed data\n",
    "    np.set_printoptions(precision=3)\n",
    "    print(normalizedX[0:5,:])\n",
    "    \n",
    "    return normalizedX\n",
    "    \n",
    "def binarize_data(X):\n",
    "    \n",
    "    '''You can transform your data using a binary threshold. All values above the threshold are marked 1 and all\n",
    "    equal to or below are marked as 0. This is called binarizing your data or threshold your data. It can be useful\n",
    "    when you have probabilities that you want to make crisp values. It is also useful when feature engineering and \n",
    "    you want to add new features that indicate something meaningful.\n",
    "    \n",
    "    Input: dataframe data to be used for features\n",
    "    Return: binarized data\n",
    "    '''\n",
    "    \n",
    "    binarizer = Binarizer(threshold=0.0).fit(X)\n",
    "    binaryX = binarizer.transform(X)\n",
    "    \n",
    "    # summarize transformed data\n",
    "    np.set_printoptions(precision=3)\n",
    "    print(binaryX[0:5,:])\n",
    "    \n",
    "    return binaryX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "# prepared_df, X, y = prepare_data_from_csv('pima-indians-diabetes.data copy.csv', 'class', column_names=names, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe_data(prepared_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resc_x = rescale_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stand_x = standardize_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm_x = normalize_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin_x = binarize_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
